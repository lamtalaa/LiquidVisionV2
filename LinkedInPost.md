
ğŸš€ Just shipped something Iâ€™m really proud of!

**What I Built**  
LiquidVision â€“ a SwiftUI iOS app that uses Core ML to classify photos and instantly gauges the sentiment of the predicted label with Appleâ€™s Natural Language framework. The latest iteration embraces a dedicated root tab view with lightweight coordinators so navigation wiring stays clean while views own presentation.

**What I Learned**  
- Orchestrating Vision + Core ML asynchronously without blocking the main thread.  
- Taming NLTagger sentiment analysis (and its concurrency quirks) for fast, user-friendly feedback.  
- Why pushing view state into a single `ViewState` valueâ€”while letting coordinators focus on dependency injectionâ€”keeps SwiftUI codebases testable and reactive.

**Tools & Stack**  
SwiftUI â€¢ PhotosUI â€¢ CameraKit (UIKit bridge) â€¢ Core ML (MobileNetV2) â€¢ Vision â€¢ Natural Language â€¢ XCTest

**Visual Snippet**  
ğŸ“¸ Imagine a glassy SwiftUI card: snap a photo â†’ see â€œHappy Dogâ€ at 92% confidence â†’ sentiment animates in as â€œPositive (0.78).â€ Flip to the Sentiment tab to demo text analysis live.

**Reflection**  
Pulling NLP and computer vision into one cohesive UX pushed me to think deeply about async state management. The biggest win? Keeping the pipeline testableâ€”from ML inference to sentiment scoringâ€”while maintaining that fluid â€œliquid glassâ€ aesthetic.

**Bonus â€“ Try It or Peek Under the Hood**  
ğŸ‘‰ GitHub: https://github.com/lamtalaa/LiquidVisionV2
